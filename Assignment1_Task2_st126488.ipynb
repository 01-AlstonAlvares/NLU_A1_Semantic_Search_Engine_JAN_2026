{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e52362b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     C:\\Users\\alsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\alsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "import requests\n",
    "from collections import Counter, defaultdict\n",
    "from scipy.stats import spearmanr\n",
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "import gensim.downloader as api\n",
    "\n",
    "# DATA LOADING & PREPROCESSING\n",
    "try:\n",
    "    nltk.download('reuters')\n",
    "    nltk.download('punkt')\n",
    "except Exception as e:\n",
    "    print(f\"NLTK Download Error: {e}\")\n",
    "\n",
    "def load_news_dataset(category='grain', max_vocab=2000):\n",
    "    raw_sents = reuters.sents(categories=category)\n",
    "    clean_corpus = [[w.lower() for w in s if w.isalpha()] for s in raw_sents]\n",
    "    all_words = [w for s in clean_corpus for w in s]\n",
    "    counts = Counter(all_words)\n",
    "    vocab = sorted(counts, key=counts.get, reverse=True)[:max_vocab]\n",
    "    vocab.append('<UNK>')\n",
    "    word2idx = {w: i for i, w in enumerate(vocab)}\n",
    "    return clean_corpus, vocab, word2idx\n",
    "\n",
    "def get_skipgrams(corpus, w2i, window_size):\n",
    "    \"\"\"Generates training pairs dynamically based on window size.\"\"\"\n",
    "    data = []\n",
    "    for sentence in corpus:\n",
    "        for i, word in enumerate(sentence):\n",
    "            if word not in w2i: continue\n",
    "            target = w2i[word]\n",
    "            start = max(0, i - window_size)\n",
    "            end = min(len(sentence), i + window_size + 1)\n",
    "            for j in range(start, end):\n",
    "                if i == j or sentence[j] not in w2i: continue\n",
    "                data.append((target, w2i[sentence[j]]))\n",
    "    return data\n",
    "\n",
    "def get_cooc_matrix(corpus, w2i, window_size):\n",
    "    \"\"\"Generates co-occurrence matrix dynamically for GloVe.\"\"\"\n",
    "    cooc = defaultdict(float)\n",
    "    for sentence in corpus:\n",
    "        for i, word in enumerate(sentence):\n",
    "            if word not in w2i: continue\n",
    "            w_i = w2i[word]\n",
    "            start = max(0, i - window_size)\n",
    "            end = min(len(sentence), i + window_size + 1)\n",
    "            for j in range(start, end):\n",
    "                if i == j or sentence[j] not in w2i: continue\n",
    "                w_j = w2i[sentence[j]]\n",
    "                cooc[(w_i, w_j)] += 1.0 / abs(i - j)\n",
    "    return [(i, j, c, min(1.0, (c/100)**0.75)) for (i, j), c in cooc.items()]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f056df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL ARCHITECTURES\n",
    "\n",
    "class SkipgramSoftmax(nn.Module): # Skip-gram with Softmax model implementation\n",
    "    def __init__(self, v_size, emb_dim):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(v_size, emb_dim)\n",
    "        self.output = nn.Linear(emb_dim, v_size)\n",
    "    def forward(self, x):\n",
    "        return self.output(self.embeddings(x))\n",
    "\n",
    "class Word2VecNeg(nn.Module): # Negative Sampling model implementation\n",
    "    def __init__(self, v_size, emb_dim):\n",
    "        super().__init__()\n",
    "        self.v_embeddings = nn.Embedding(v_size, emb_dim)\n",
    "        self.u_embeddings = nn.Embedding(v_size, emb_dim)\n",
    "        self.log_sigmoid = nn.LogSigmoid()\n",
    "    def forward(self, center, target, negative):\n",
    "        v = self.v_embeddings(center).view(center.size(0), 1, -1)\n",
    "        u = self.u_embeddings(target).view(target.size(0), 1, -1)\n",
    "        n = -self.u_embeddings(negative)\n",
    "        pos = torch.bmm(u, v.transpose(1, 2)).view(center.size(0), -1)\n",
    "        neg = torch.bmm(n, v.transpose(1, 2)).squeeze(2)\n",
    "        return -torch.mean(self.log_sigmoid(pos) + torch.sum(self.log_sigmoid(neg), 1))\n",
    "\n",
    "class GloVeModel(nn.Module): # Glove model implementation\n",
    "    def __init__(self, v_size, emb_dim):\n",
    "        super().__init__()\n",
    "        self.v_emb, self.u_emb = nn.Embedding(v_size, emb_dim), nn.Embedding(v_size, emb_dim)\n",
    "        self.v_bias, self.u_bias = nn.Embedding(v_size, 1), nn.Embedding(v_size, 1)\n",
    "    def forward(self, i, j, cooc, weights):\n",
    "        dot = (self.v_emb(i) * self.u_emb(j)).sum(1)\n",
    "        b_i, b_j = self.v_bias(i).squeeze(), self.u_bias(j).squeeze()\n",
    "        return torch.mean(weights * torch.pow(dot + b_i + b_j - torch.log(cooc), 2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17567ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. EVALUATION LOGIC (TASK 2) ---\n",
    "\n",
    "def evaluate_spearman(model, w2i, csv_path, is_gensim=False):\n",
    "    \"\"\"Calculates correlation between model dot products and human metrics.\"\"\"\n",
    "    if not os.path.exists(csv_path): return 0.0\n",
    "    df = pd.read_csv(csv_path)\n",
    "    if is_gensim:\n",
    "        vecs = model\n",
    "    else:\n",
    "        if hasattr(model, 'v_embeddings'): v = model.v_embeddings.weight.detach().numpy()\n",
    "        elif hasattr(model, 'v_emb'): v = (model.v_emb.weight + model.u_emb.weight).detach().numpy()\n",
    "        else: v = model.embeddings.weight.detach().numpy()\n",
    "        vecs = v\n",
    "    m_sims, h_scores = [], []\n",
    "    for _, row in df.iterrows():\n",
    "        w1, w2, h_score = str(row['Word 1']).lower(), str(row['Word 2']).lower(), float(row['Human (mean)'])\n",
    "        if is_gensim:\n",
    "            if w1 in vecs and w2 in vecs:\n",
    "                m_sims.append(np.dot(vecs[w1], vecs[w2]))\n",
    "                h_scores.append(h_score)\n",
    "        elif w1 in w2i and w2 in w2i:\n",
    "            v1, v2 = vecs[w2i[w1]], vecs[w2i[w2]]\n",
    "            m_sims.append(np.dot(v1, v2))\n",
    "            h_scores.append(h_score)\n",
    "    corr, _ = spearmanr(m_sims, h_scores)\n",
    "    return corr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1557e83e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " TESTING WINDOW SIZE: 2 \n",
      "  Skipgram Softmax (W=2) Epoch 1: Loss 6.0923\n",
      "  Skipgram Softmax (W=2) Epoch 2: Loss 5.6231\n",
      "  Skipgram Softmax (W=2) Epoch 3: Loss 5.4678\n",
      "  Skipgram Softmax (W=2) Epoch 4: Loss 5.3722\n",
      "  Skipgram Softmax (W=2) Epoch 5: Loss 5.3086\n",
      "  Skipgram Softmax (W=2) Epoch 6: Loss 5.2608\n",
      "  Skipgram Softmax (W=2) Epoch 7: Loss 5.2263\n",
      "  Skipgram Softmax (W=2) Epoch 8: Loss 5.1998\n",
      "  Skipgram Softmax (W=2) Epoch 9: Loss 5.1798\n",
      "  Skipgram Softmax (W=2) Epoch 10: Loss 5.1627\n",
      "  Skipgram Softmax (W=2) Epoch 11: Loss 5.1493\n",
      "  Skipgram Softmax (W=2) Epoch 12: Loss 5.1385\n",
      "  Skipgram Softmax (W=2) Epoch 13: Loss 5.1298\n",
      "  Skipgram Softmax (W=2) Epoch 14: Loss 5.1222\n",
      "  Skipgram Softmax (W=2) Epoch 15: Loss 5.1162\n",
      "  Skipgram Softmax (W=2) Epoch 16: Loss 5.1107\n",
      "  Skipgram Softmax (W=2) Epoch 17: Loss 5.1063\n",
      "  Skipgram Softmax (W=2) Epoch 18: Loss 5.1018\n",
      "  Skipgram Softmax (W=2) Epoch 19: Loss 5.0987\n",
      "  Skipgram Softmax (W=2) Epoch 20: Loss 5.0967\n",
      "  Skipgram NEG (W=2) Epoch 1: Loss 9.9086\n",
      "  Skipgram NEG (W=2) Epoch 2: Loss 2.8750\n",
      "  Skipgram NEG (W=2) Epoch 3: Loss 1.9587\n",
      "  Skipgram NEG (W=2) Epoch 4: Loss 1.6296\n",
      "  Skipgram NEG (W=2) Epoch 5: Loss 1.4821\n",
      "  Skipgram NEG (W=2) Epoch 6: Loss 1.3965\n",
      "  Skipgram NEG (W=2) Epoch 7: Loss 1.3519\n",
      "  Skipgram NEG (W=2) Epoch 8: Loss 1.3202\n",
      "  Skipgram NEG (W=2) Epoch 9: Loss 1.2958\n",
      "  Skipgram NEG (W=2) Epoch 10: Loss 1.2764\n",
      "  Skipgram NEG (W=2) Epoch 11: Loss 1.2640\n",
      "  Skipgram NEG (W=2) Epoch 12: Loss 1.2548\n",
      "  Skipgram NEG (W=2) Epoch 13: Loss 1.2444\n",
      "  Skipgram NEG (W=2) Epoch 14: Loss 1.2387\n",
      "  Skipgram NEG (W=2) Epoch 15: Loss 1.2317\n",
      "  Skipgram NEG (W=2) Epoch 16: Loss 1.2235\n",
      "  Skipgram NEG (W=2) Epoch 17: Loss 1.2200\n",
      "  Skipgram NEG (W=2) Epoch 18: Loss 1.2175\n",
      "  Skipgram NEG (W=2) Epoch 19: Loss 1.2146\n",
      "  Skipgram NEG (W=2) Epoch 20: Loss 1.2138\n",
      "  GloVe Scratch (W=2) Epoch 1: Loss 4.4869\n",
      "  GloVe Scratch (W=2) Epoch 2: Loss 2.1256\n",
      "  GloVe Scratch (W=2) Epoch 3: Loss 0.9153\n",
      "  GloVe Scratch (W=2) Epoch 4: Loss 0.4522\n",
      "  GloVe Scratch (W=2) Epoch 5: Loss 0.2567\n",
      "  GloVe Scratch (W=2) Epoch 6: Loss 0.1746\n",
      "  GloVe Scratch (W=2) Epoch 7: Loss 0.1431\n",
      "  GloVe Scratch (W=2) Epoch 8: Loss 0.1381\n",
      "  GloVe Scratch (W=2) Epoch 9: Loss 0.1506\n",
      "  GloVe Scratch (W=2) Epoch 10: Loss 0.1571\n",
      "  GloVe Scratch (W=2) Epoch 11: Loss 0.1371\n",
      "  GloVe Scratch (W=2) Epoch 12: Loss 0.1196\n",
      "  GloVe Scratch (W=2) Epoch 13: Loss 0.1145\n",
      "  GloVe Scratch (W=2) Epoch 14: Loss 0.1146\n",
      "  GloVe Scratch (W=2) Epoch 15: Loss 0.1128\n",
      "  GloVe Scratch (W=2) Epoch 16: Loss 0.1082\n",
      "  GloVe Scratch (W=2) Epoch 17: Loss 0.1020\n",
      "  GloVe Scratch (W=2) Epoch 18: Loss 0.0984\n",
      "  GloVe Scratch (W=2) Epoch 19: Loss 0.0953\n",
      "  GloVe Scratch (W=2) Epoch 20: Loss 0.0930\n",
      "\n",
      " TESTING WINDOW SIZE: 5 \n",
      "  Skipgram Softmax (W=5) Epoch 1: Loss 6.2008\n",
      "  Skipgram Softmax (W=5) Epoch 2: Loss 5.9051\n",
      "  Skipgram Softmax (W=5) Epoch 3: Loss 5.8073\n",
      "  Skipgram Softmax (W=5) Epoch 4: Loss 5.7493\n",
      "  Skipgram Softmax (W=5) Epoch 5: Loss 5.7136\n",
      "  Skipgram Softmax (W=5) Epoch 6: Loss 5.6903\n",
      "  Skipgram Softmax (W=5) Epoch 7: Loss 5.6753\n",
      "  Skipgram Softmax (W=5) Epoch 8: Loss 5.6638\n",
      "  Skipgram Softmax (W=5) Epoch 9: Loss 5.6565\n",
      "  Skipgram Softmax (W=5) Epoch 10: Loss 5.6499\n",
      "  Skipgram Softmax (W=5) Epoch 11: Loss 5.6463\n",
      "  Skipgram Softmax (W=5) Epoch 12: Loss 5.6420\n",
      "  Skipgram Softmax (W=5) Epoch 13: Loss 5.6401\n",
      "  Skipgram Softmax (W=5) Epoch 14: Loss 5.6372\n",
      "  Skipgram Softmax (W=5) Epoch 15: Loss 5.6361\n",
      "  Skipgram Softmax (W=5) Epoch 16: Loss 5.6345\n",
      "  Skipgram Softmax (W=5) Epoch 17: Loss 5.6336\n",
      "  Skipgram Softmax (W=5) Epoch 18: Loss 5.6325\n",
      "  Skipgram Softmax (W=5) Epoch 19: Loss 5.6322\n",
      "  Skipgram Softmax (W=5) Epoch 20: Loss 5.6312\n",
      "  Skipgram NEG (W=5) Epoch 1: Loss 6.3335\n",
      "  Skipgram NEG (W=5) Epoch 2: Loss 2.1078\n",
      "  Skipgram NEG (W=5) Epoch 3: Loss 1.7850\n",
      "  Skipgram NEG (W=5) Epoch 4: Loss 1.7004\n",
      "  Skipgram NEG (W=5) Epoch 5: Loss 1.6618\n",
      "  Skipgram NEG (W=5) Epoch 6: Loss 1.6369\n",
      "  Skipgram NEG (W=5) Epoch 7: Loss 1.6186\n",
      "  Skipgram NEG (W=5) Epoch 8: Loss 1.6049\n",
      "  Skipgram NEG (W=5) Epoch 9: Loss 1.5945\n",
      "  Skipgram NEG (W=5) Epoch 10: Loss 1.5822\n",
      "  Skipgram NEG (W=5) Epoch 11: Loss 1.5761\n",
      "  Skipgram NEG (W=5) Epoch 12: Loss 1.5715\n",
      "  Skipgram NEG (W=5) Epoch 13: Loss 1.5645\n",
      "  Skipgram NEG (W=5) Epoch 14: Loss 1.5601\n",
      "  Skipgram NEG (W=5) Epoch 15: Loss 1.5583\n",
      "  Skipgram NEG (W=5) Epoch 16: Loss 1.5538\n",
      "  Skipgram NEG (W=5) Epoch 17: Loss 1.5505\n",
      "  Skipgram NEG (W=5) Epoch 18: Loss 1.5471\n",
      "  Skipgram NEG (W=5) Epoch 19: Loss 1.5452\n",
      "  Skipgram NEG (W=5) Epoch 20: Loss 1.5398\n",
      "  GloVe Scratch (W=5) Epoch 1: Loss 3.1392\n",
      "  GloVe Scratch (W=5) Epoch 2: Loss 1.2182\n",
      "  GloVe Scratch (W=5) Epoch 3: Loss 0.4190\n",
      "  GloVe Scratch (W=5) Epoch 4: Loss 0.1776\n",
      "  GloVe Scratch (W=5) Epoch 5: Loss 0.1064\n",
      "  GloVe Scratch (W=5) Epoch 6: Loss 0.0916\n",
      "  GloVe Scratch (W=5) Epoch 7: Loss 0.0946\n",
      "  GloVe Scratch (W=5) Epoch 8: Loss 0.0851\n",
      "  GloVe Scratch (W=5) Epoch 9: Loss 0.0751\n",
      "  GloVe Scratch (W=5) Epoch 10: Loss 0.0711\n",
      "  GloVe Scratch (W=5) Epoch 11: Loss 0.0676\n",
      "  GloVe Scratch (W=5) Epoch 12: Loss 0.0632\n",
      "  GloVe Scratch (W=5) Epoch 13: Loss 0.0592\n",
      "  GloVe Scratch (W=5) Epoch 14: Loss 0.0562\n",
      "  GloVe Scratch (W=5) Epoch 15: Loss 0.0531\n",
      "  GloVe Scratch (W=5) Epoch 16: Loss 0.0502\n",
      "  GloVe Scratch (W=5) Epoch 17: Loss 0.0476\n",
      "  GloVe Scratch (W=5) Epoch 18: Loss 0.0450\n",
      "  GloVe Scratch (W=5) Epoch 19: Loss 0.0426\n",
      "  GloVe Scratch (W=5) Epoch 20: Loss 0.0405\n",
      "\n",
      " TESTING WINDOW SIZE: 10 \n",
      "  Skipgram Softmax (W=10) Epoch 1: Loss 6.1896\n",
      "  Skipgram Softmax (W=10) Epoch 2: Loss 5.9625\n",
      "  Skipgram Softmax (W=10) Epoch 3: Loss 5.9040\n",
      "  Skipgram Softmax (W=10) Epoch 4: Loss 5.8735\n",
      "  Skipgram Softmax (W=10) Epoch 5: Loss 5.8561\n",
      "  Skipgram Softmax (W=10) Epoch 6: Loss 5.8445\n",
      "  Skipgram Softmax (W=10) Epoch 7: Loss 5.8372\n",
      "  Skipgram Softmax (W=10) Epoch 8: Loss 5.8315\n",
      "  Skipgram Softmax (W=10) Epoch 9: Loss 5.8280\n",
      "  Skipgram Softmax (W=10) Epoch 10: Loss 5.8244\n",
      "  Skipgram Softmax (W=10) Epoch 11: Loss 5.8225\n",
      "  Skipgram Softmax (W=10) Epoch 12: Loss 5.8202\n",
      "  Skipgram Softmax (W=10) Epoch 13: Loss 5.8189\n",
      "  Skipgram Softmax (W=10) Epoch 14: Loss 5.8171\n",
      "  Skipgram Softmax (W=10) Epoch 15: Loss 5.8167\n",
      "  Skipgram Softmax (W=10) Epoch 16: Loss 5.8158\n",
      "  Skipgram Softmax (W=10) Epoch 17: Loss 5.8157\n",
      "  Skipgram Softmax (W=10) Epoch 18: Loss 5.8155\n",
      "  Skipgram Softmax (W=10) Epoch 19: Loss 5.8153\n",
      "  Skipgram Softmax (W=10) Epoch 20: Loss 5.8147\n",
      "  Skipgram NEG (W=10) Epoch 1: Loss 4.8522\n",
      "  Skipgram NEG (W=10) Epoch 2: Loss 1.9739\n",
      "  Skipgram NEG (W=10) Epoch 3: Loss 1.8659\n",
      "  Skipgram NEG (W=10) Epoch 4: Loss 1.8293\n",
      "  Skipgram NEG (W=10) Epoch 5: Loss 1.8076\n",
      "  Skipgram NEG (W=10) Epoch 6: Loss 1.7865\n",
      "  Skipgram NEG (W=10) Epoch 7: Loss 1.7701\n",
      "  Skipgram NEG (W=10) Epoch 8: Loss 1.7557\n",
      "  Skipgram NEG (W=10) Epoch 9: Loss 1.7437\n",
      "  Skipgram NEG (W=10) Epoch 10: Loss 1.7326\n",
      "  Skipgram NEG (W=10) Epoch 11: Loss 1.7273\n",
      "  Skipgram NEG (W=10) Epoch 12: Loss 1.7222\n",
      "  Skipgram NEG (W=10) Epoch 13: Loss 1.7184\n",
      "  Skipgram NEG (W=10) Epoch 14: Loss 1.7170\n",
      "  Skipgram NEG (W=10) Epoch 15: Loss 1.7212\n",
      "  Skipgram NEG (W=10) Epoch 16: Loss 1.7230\n",
      "  Skipgram NEG (W=10) Epoch 17: Loss 1.7254\n",
      "  Skipgram NEG (W=10) Epoch 18: Loss 1.7302\n",
      "  Skipgram NEG (W=10) Epoch 19: Loss 1.7321\n",
      "  Skipgram NEG (W=10) Epoch 20: Loss 1.7331\n",
      "  GloVe Scratch (W=10) Epoch 1: Loss 2.4052\n",
      "  GloVe Scratch (W=10) Epoch 2: Loss 0.7852\n",
      "  GloVe Scratch (W=10) Epoch 3: Loss 0.2206\n",
      "  GloVe Scratch (W=10) Epoch 4: Loss 0.0910\n",
      "  GloVe Scratch (W=10) Epoch 5: Loss 0.0678\n",
      "  GloVe Scratch (W=10) Epoch 6: Loss 0.0638\n",
      "  GloVe Scratch (W=10) Epoch 7: Loss 0.0577\n",
      "  GloVe Scratch (W=10) Epoch 8: Loss 0.0517\n",
      "  GloVe Scratch (W=10) Epoch 9: Loss 0.0482\n",
      "  GloVe Scratch (W=10) Epoch 10: Loss 0.0444\n",
      "  GloVe Scratch (W=10) Epoch 11: Loss 0.0416\n",
      "  GloVe Scratch (W=10) Epoch 12: Loss 0.0386\n",
      "  GloVe Scratch (W=10) Epoch 13: Loss 0.0366\n",
      "  GloVe Scratch (W=10) Epoch 14: Loss 0.0341\n",
      "  GloVe Scratch (W=10) Epoch 15: Loss 0.0323\n",
      "  GloVe Scratch (W=10) Epoch 16: Loss 0.0305\n",
      "  GloVe Scratch (W=10) Epoch 17: Loss 0.0288\n",
      "  GloVe Scratch (W=10) Epoch 18: Loss 0.0275\n",
      "  GloVe Scratch (W=10) Epoch 19: Loss 0.0260\n",
      "  GloVe Scratch (W=10) Epoch 20: Loss 0.0248\n",
      "\n",
      "Evaluating GloVe (Gensim)\n",
      "\n",
      " PERFORMANCE SUMMARY TABLE \n",
      "           Model Window Training Time Spearman Rho\n",
      "Skipgram Softmax      2       105.75s       0.1791\n",
      "    Skipgram NEG      2        86.34s       0.0451\n",
      "   GloVe Scratch      2        20.39s      -0.2526\n",
      "Skipgram Softmax      5       240.69s       0.2890\n",
      "    Skipgram NEG      5       206.64s       0.0268\n",
      "   GloVe Scratch      5        54.90s      -0.0660\n",
      "Skipgram Softmax     10       432.95s       0.1206\n",
      "    Skipgram NEG     10       374.44s       0.1473\n",
      "   GloVe Scratch     10        72.01s      -0.0478\n",
      "  GloVe (gensim)    N/A           N/A       0.4924\n",
      "\n",
      " TABLE 1: ASSESSMENT OF HUMAN JUDGMENT CORRELATION\n",
      "              Skipgram       NEG    GloVe  GloVe (gensim)  Y_true\n",
      "Spearman Rho  0.289006  0.147271 -0.04776        0.492414     1.0\n"
     ]
    }
   ],
   "source": [
    "# EXECUTION LOOP FOR MULTIPLE WINDOW SIZES \n",
    "\n",
    "WINDOW_SIZES = [2, 5, 10]\n",
    "corpus, vocab, word2idx = load_news_dataset()\n",
    "performance_results = []\n",
    "judgment_results = {}\n",
    "\n",
    "for window in WINDOW_SIZES:\n",
    "    print(f\"\\n TESTING WINDOW SIZE: {window} \")\n",
    "    \n",
    "    models = [\n",
    "        (\"Skipgram Softmax\", SkipgramSoftmax(len(vocab), 100)),\n",
    "        (\"Skipgram NEG\", Word2VecNeg(len(vocab), 100)),\n",
    "        (\"GloVe Scratch\", GloVeModel(len(vocab), 100))\n",
    "    ]\n",
    "\n",
    "    for name, model in models:\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Dynamic Data Generation and Training with Epoch Logging\n",
    "        if \"Skipgram\" in name:\n",
    "            pairs = get_skipgrams(corpus, word2idx, window)\n",
    "            criterion = nn.CrossEntropyLoss() if \"Softmax\" in name else None\n",
    "            for epoch in range(20): \n",
    "                total_loss = 0\n",
    "                for i in range(0, len(pairs), 128):\n",
    "                    batch = pairs[i:i+128]\n",
    "                    if len(batch) < 128: continue\n",
    "                    c, t = torch.LongTensor([p[0] for p in batch]), torch.LongTensor([p[1] for p in batch])\n",
    "                    optimizer.zero_grad()\n",
    "                    loss = model(c, t, torch.LongTensor(np.random.randint(0, len(vocab), (128, 5)))) if \"NEG\" in name else criterion(model(c), t)\n",
    "                    loss.backward(); optimizer.step(); total_loss += loss.item()\n",
    "                avg_loss = total_loss / (len(pairs)//128)\n",
    "                print(f\"  {name} (W={window}) Epoch {epoch+1}: Loss {avg_loss:.4f}\")\n",
    "        else: # GloVe\n",
    "            cooc = get_cooc_matrix(corpus, word2idx, window)\n",
    "            for epoch in range(20):\n",
    "                total_loss = 0\n",
    "                for i in range(0, len(cooc), 128):\n",
    "                    batch = cooc[i:i+128]\n",
    "                    if len(batch) < 128: continue\n",
    "                    idx_i, idx_j = torch.LongTensor([p[0] for p in batch]), torch.LongTensor([p[1] for p in batch])\n",
    "                    cnt, wgt = torch.FloatTensor([p[2] for p in batch]), torch.FloatTensor([p[3] for p in batch])\n",
    "                    optimizer.zero_grad(); loss = model(idx_i, idx_j, cnt, wgt); loss.backward(); optimizer.step(); total_loss += loss.item()\n",
    "                avg_loss = total_loss / (len(cooc)//128)\n",
    "                print(f\"  {name} (W={window}) Epoch {epoch+1}: Loss {avg_loss:.4f}\")\n",
    "\n",
    "        duration = time.time() - start_time\n",
    "        corr = evaluate_spearman(model, word2idx, 'combined.csv')\n",
    "        \n",
    "        performance_results.append({\n",
    "            \"Model\": name, \"Window\": window, \n",
    "            \"Training Time\": f\"{duration:.2f}s\", \"Spearman Rho\": f\"{corr:.4f}\"\n",
    "        })\n",
    "        \n",
    "        # Keep best Spearman score per model type for the Assessment table\n",
    "        if name not in judgment_results or corr > judgment_results[name]:\n",
    "            judgment_results[name] = corr\n",
    "\n",
    "# Pre-trained Gensim Benchmark\n",
    "print(\"\\nEvaluating GloVe (Gensim)\")\n",
    "g_g = api.load(\"glove-wiki-gigaword-100\")\n",
    "g_corr = evaluate_spearman(g_g, None, 'combined.csv', is_gensim=True)\n",
    "performance_results.append({\"Model\": \"GloVe (gensim)\", \"Window\": \"N/A\", \"Training Time\": \"N/A\", \"Spearman Rho\": f\"{g_corr:.4f}\"})\n",
    "judgment_results[\"GloVe (gensim)\"] = g_corr\n",
    "\n",
    "# DISPLAY RESULTS\n",
    "print(\"\\n PERFORMANCE SUMMARY TABLE \")\n",
    "print(pd.DataFrame(performance_results).to_string(index=False))\n",
    "\n",
    "swapped_data = [[\n",
    "    judgment_results.get(\"Skipgram Softmax\", 0),\n",
    "    judgment_results.get(\"Skipgram NEG\", 0),\n",
    "    judgment_results.get(\"GloVe Scratch\", 0),\n",
    "    judgment_results.get(\"GloVe (gensim)\", 0),\n",
    "    1.0 # Y_true benchmark\n",
    "]]\n",
    "print(\"\\n TABLE 1: ASSESSMENT OF HUMAN JUDGMENT CORRELATION\")\n",
    "print(pd.DataFrame(swapped_data, columns=[\"Skipgram\", \"NEG\", \"GloVe\", \"GloVe (gensim)\", \"Y_true\"], index=[\"Spearman Rho\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
