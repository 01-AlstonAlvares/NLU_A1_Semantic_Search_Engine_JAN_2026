{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10809e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "import requests\n",
    "from collections import Counter, defaultdict\n",
    "from scipy.stats import spearmanr\n",
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "import gensim.downloader as api\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1df221d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     C:\\Users\\alsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\alsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# DATA LOADING & PREPROCESSING\n",
    "try:\n",
    "    nltk.download('reuters')\n",
    "    nltk.download('punkt')\n",
    "except Exception as e:\n",
    "    print(f\"NLTK Download Error: {e}\")\n",
    "\n",
    "def load_news_dataset(category='grain', max_vocab=2000):\n",
    "    raw_sents = reuters.sents(categories=category)\n",
    "    clean_corpus = [[w.lower() for w in s if w.isalpha()] for s in raw_sents]\n",
    "    all_words = [w for s in clean_corpus for w in s]\n",
    "    counts = Counter(all_words)\n",
    "    vocab = sorted(counts, key=counts.get, reverse=True)[:max_vocab]\n",
    "    vocab.append('<UNK>')\n",
    "    word2idx = {w: i for i, w in enumerate(vocab)}\n",
    "    return clean_corpus, vocab, word2idx\n",
    "\n",
    "def get_skipgrams(corpus, w2i, window_size):\n",
    "    \"\"\"Generates training pairs dynamically based on window size.\"\"\"\n",
    "    data = []\n",
    "    for sentence in corpus:\n",
    "        for i, word in enumerate(sentence):\n",
    "            if word not in w2i: continue\n",
    "            target = w2i[word]\n",
    "            start = max(0, i - window_size)\n",
    "            end = min(len(sentence), i + window_size + 1)\n",
    "            for j in range(start, end):\n",
    "                if i == j or sentence[j] not in w2i: continue\n",
    "                data.append((target, w2i[sentence[j]]))\n",
    "    return data\n",
    "\n",
    "def get_cooc_matrix(corpus, w2i, window_size):\n",
    "    \"\"\"Generates co-occurrence matrix dynamically for GloVe.\"\"\"\n",
    "    cooc = defaultdict(float)\n",
    "    for sentence in corpus:\n",
    "        for i, word in enumerate(sentence):\n",
    "            if word not in w2i: continue\n",
    "            w_i = w2i[word]\n",
    "            start = max(0, i - window_size)\n",
    "            end = min(len(sentence), i + window_size + 1)\n",
    "            for j in range(start, end):\n",
    "                if i == j or sentence[j] not in w2i: continue\n",
    "                w_j = w2i[sentence[j]]\n",
    "                cooc[(w_i, w_j)] += 1.0 / abs(i - j)\n",
    "    return [(i, j, c, min(1.0, (c/100)**0.75)) for (i, j), c in cooc.items()]\n",
    "\n",
    "def download_analogy_data():\n",
    "    url = \"http://download.tensorflow.org/data/questions-words.txt\"\n",
    "    path = \"questions-words.txt\"\n",
    "    if not os.path.exists(path):\n",
    "        print(\"Downloading Analogy Dataset...\")\n",
    "        r = requests.get(url)\n",
    "        with open(path, 'wb') as f:\n",
    "            f.write(r.content)\n",
    "    return path\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87749f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL ARCHITECTURES\n",
    "\n",
    "class SkipgramSoftmax(nn.Module): \n",
    "    def __init__(self, v_size, emb_dim):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(v_size, emb_dim)\n",
    "        self.output = nn.Linear(emb_dim, v_size)\n",
    "    def forward(self, x):\n",
    "        return self.output(self.embeddings(x))\n",
    "\n",
    "class Word2VecNeg(nn.Module): \n",
    "    def __init__(self, v_size, emb_dim):\n",
    "        super().__init__()\n",
    "        self.v_embeddings = nn.Embedding(v_size, emb_dim)\n",
    "        self.u_embeddings = nn.Embedding(v_size, emb_dim)\n",
    "        self.log_sigmoid = nn.LogSigmoid()\n",
    "    def forward(self, center, target, negative):\n",
    "        v = self.v_embeddings(center).view(center.size(0), 1, -1)\n",
    "        u = self.u_embeddings(target).view(target.size(0), 1, -1)\n",
    "        n = -self.u_embeddings(negative)\n",
    "        pos = torch.bmm(u, v.transpose(1, 2)).view(center.size(0), -1)\n",
    "        neg = torch.bmm(n, v.transpose(1, 2)).squeeze(2)\n",
    "        return -torch.mean(self.log_sigmoid(pos) + torch.sum(self.log_sigmoid(neg), 1))\n",
    "\n",
    "class GloVeModel(nn.Module): \n",
    "    def __init__(self, v_size, emb_dim):\n",
    "        super().__init__()\n",
    "        self.v_emb, self.u_emb = nn.Embedding(v_size, emb_dim), nn.Embedding(v_size, emb_dim)\n",
    "        self.v_bias, self.u_bias = nn.Embedding(v_size, 1), nn.Embedding(v_size, 1)\n",
    "    def forward(self, i, j, cooc, weights):\n",
    "        dot = (self.v_emb(i) * self.u_emb(j)).sum(1)\n",
    "        b_i, b_j = self.v_bias(i).squeeze(), self.u_bias(j).squeeze()\n",
    "        return torch.mean(weights * torch.pow(dot + b_i + b_j - torch.log(cooc), 2))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66a7b68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVALUATION LOGIC\n",
    "\n",
    "def evaluate_analogy(vectors, w2i, i2w, analogy_path, category_name):\n",
    "    correct, total = 0, 0\n",
    "    if not os.path.exists(analogy_path): return 0.0\n",
    "    with open(analogy_path, 'r') as f:\n",
    "        target_section = False\n",
    "        for line in f:\n",
    "            if line.startswith(':'):\n",
    "                target_section = category_name in line\n",
    "                continue\n",
    "            if not target_section: continue\n",
    "            words = line.strip().lower().split()\n",
    "            if len(words) != 4: continue\n",
    "            a, b, c, d = words\n",
    "            if all(w in w2i for w in words):\n",
    "                total += 1\n",
    "                va, vb, vc = vectors[w2i[a]], vectors[w2i[b]], vectors[w2i[c]]\n",
    "                target_vec = vb - va + vc\n",
    "                sims = np.dot(vectors, target_vec) / (np.linalg.norm(vectors, axis=1) * np.linalg.norm(target_vec) + 1e-10)\n",
    "                sims[w2i[a]] = sims[w2i[b]] = sims[w2i[c]] = -1\n",
    "                pred = i2w[np.argmax(sims)]\n",
    "                if pred == d: correct += 1\n",
    "    return (correct / total) * 100 if total > 0 else 0.0\n",
    "\n",
    "def evaluate_spearman(model, w2i, csv_path, is_gensim=False):\n",
    "    if not os.path.exists(csv_path): return 0.0\n",
    "    df = pd.read_csv(csv_path)\n",
    "    if is_gensim:\n",
    "        vecs = model\n",
    "    else:\n",
    "        if hasattr(model, 'v_embeddings'): v = model.v_embeddings.weight.detach().numpy()\n",
    "        elif hasattr(model, 'v_emb'): v = (model.v_emb.weight + model.u_emb.weight).detach().numpy()\n",
    "        else: v = model.embeddings.weight.detach().numpy()\n",
    "        vecs = v\n",
    "    m_sims, h_scores = [], []\n",
    "    for _, row in df.iterrows():\n",
    "        w1, w2, h_score = str(row['Word 1']).lower(), str(row['Word 2']).lower(), float(row['Human (mean)'])\n",
    "        if is_gensim:\n",
    "            if w1 in vecs and w2 in vecs:\n",
    "                m_sims.append(np.dot(vecs[w1], vecs[w2]))\n",
    "                h_scores.append(h_score)\n",
    "        elif w1 in w2i and w2 in w2i:\n",
    "            v1, v2 = vecs[w2i[w1]], vecs[w2i[w2]]\n",
    "            m_sims.append(np.dot(v1, v2))\n",
    "            h_scores.append(h_score)\n",
    "    if len(m_sims) < 2: return 0.0\n",
    "    corr, _ = spearmanr(m_sims, h_scores)\n",
    "    return corr\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "473ab29b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " TESTING WINDOW SIZE: 2 \n",
      "  Skipgram Softmax (W=2) Epoch 1: Loss 6.0906\n",
      "  Skipgram Softmax (W=2) Epoch 2: Loss 5.6272\n",
      "  Skipgram Softmax (W=2) Epoch 3: Loss 5.4717\n",
      "  Skipgram Softmax (W=2) Epoch 4: Loss 5.3762\n",
      "  Skipgram Softmax (W=2) Epoch 5: Loss 5.3095\n",
      "  Skipgram Softmax (W=2) Epoch 6: Loss 5.2631\n",
      "  Skipgram Softmax (W=2) Epoch 7: Loss 5.2283\n",
      "  Skipgram Softmax (W=2) Epoch 8: Loss 5.2012\n",
      "  Skipgram Softmax (W=2) Epoch 9: Loss 5.1792\n",
      "  Skipgram Softmax (W=2) Epoch 10: Loss 5.1637\n",
      "  Skipgram Softmax (W=2) Epoch 11: Loss 5.1494\n",
      "  Skipgram Softmax (W=2) Epoch 12: Loss 5.1381\n",
      "  Skipgram Softmax (W=2) Epoch 13: Loss 5.1288\n",
      "  Skipgram Softmax (W=2) Epoch 14: Loss 5.1221\n",
      "  Skipgram Softmax (W=2) Epoch 15: Loss 5.1150\n",
      "  Skipgram Softmax (W=2) Epoch 16: Loss 5.1111\n",
      "  Skipgram Softmax (W=2) Epoch 17: Loss 5.1047\n",
      "  Skipgram Softmax (W=2) Epoch 18: Loss 5.1017\n",
      "  Skipgram Softmax (W=2) Epoch 19: Loss 5.0983\n",
      "  Skipgram Softmax (W=2) Epoch 20: Loss 5.0954\n",
      "  Skipgram NEG (W=2) Epoch 1: Loss 9.8150\n",
      "  Skipgram NEG (W=2) Epoch 2: Loss 2.8653\n",
      "  Skipgram NEG (W=2) Epoch 3: Loss 1.9517\n",
      "  Skipgram NEG (W=2) Epoch 4: Loss 1.6283\n",
      "  Skipgram NEG (W=2) Epoch 5: Loss 1.4803\n",
      "  Skipgram NEG (W=2) Epoch 6: Loss 1.3968\n",
      "  Skipgram NEG (W=2) Epoch 7: Loss 1.3498\n",
      "  Skipgram NEG (W=2) Epoch 8: Loss 1.3189\n",
      "  Skipgram NEG (W=2) Epoch 9: Loss 1.2959\n",
      "  Skipgram NEG (W=2) Epoch 10: Loss 1.2772\n",
      "  Skipgram NEG (W=2) Epoch 11: Loss 1.2643\n",
      "  Skipgram NEG (W=2) Epoch 12: Loss 1.2481\n",
      "  Skipgram NEG (W=2) Epoch 13: Loss 1.2427\n",
      "  Skipgram NEG (W=2) Epoch 14: Loss 1.2381\n",
      "  Skipgram NEG (W=2) Epoch 15: Loss 1.2336\n",
      "  Skipgram NEG (W=2) Epoch 16: Loss 1.2239\n",
      "  Skipgram NEG (W=2) Epoch 17: Loss 1.2237\n",
      "  Skipgram NEG (W=2) Epoch 18: Loss 1.2172\n",
      "  Skipgram NEG (W=2) Epoch 19: Loss 1.2132\n",
      "  Skipgram NEG (W=2) Epoch 20: Loss 1.2112\n",
      "  GloVe Scratch (W=2) Epoch 1: Loss 4.3536\n",
      "  GloVe Scratch (W=2) Epoch 2: Loss 2.0457\n",
      "  GloVe Scratch (W=2) Epoch 3: Loss 0.8794\n",
      "  GloVe Scratch (W=2) Epoch 4: Loss 0.4356\n",
      "  GloVe Scratch (W=2) Epoch 5: Loss 0.2506\n",
      "  GloVe Scratch (W=2) Epoch 6: Loss 0.1743\n",
      "  GloVe Scratch (W=2) Epoch 7: Loss 0.1400\n",
      "  GloVe Scratch (W=2) Epoch 8: Loss 0.1382\n",
      "  GloVe Scratch (W=2) Epoch 9: Loss 0.1557\n",
      "  GloVe Scratch (W=2) Epoch 10: Loss 0.1591\n",
      "  GloVe Scratch (W=2) Epoch 11: Loss 0.1367\n",
      "  GloVe Scratch (W=2) Epoch 12: Loss 0.1196\n",
      "  GloVe Scratch (W=2) Epoch 13: Loss 0.1149\n",
      "  GloVe Scratch (W=2) Epoch 14: Loss 0.1159\n",
      "  GloVe Scratch (W=2) Epoch 15: Loss 0.1147\n",
      "  GloVe Scratch (W=2) Epoch 16: Loss 0.1099\n",
      "  GloVe Scratch (W=2) Epoch 17: Loss 0.1036\n",
      "  GloVe Scratch (W=2) Epoch 18: Loss 0.0999\n",
      "  GloVe Scratch (W=2) Epoch 19: Loss 0.0967\n",
      "  GloVe Scratch (W=2) Epoch 20: Loss 0.0945\n",
      "\n",
      " TESTING WINDOW SIZE: 5 \n",
      "  Skipgram Softmax (W=5) Epoch 1: Loss 6.1989\n",
      "  Skipgram Softmax (W=5) Epoch 2: Loss 5.9045\n",
      "  Skipgram Softmax (W=5) Epoch 3: Loss 5.8066\n",
      "  Skipgram Softmax (W=5) Epoch 4: Loss 5.7485\n",
      "  Skipgram Softmax (W=5) Epoch 5: Loss 5.7137\n",
      "  Skipgram Softmax (W=5) Epoch 6: Loss 5.6896\n",
      "  Skipgram Softmax (W=5) Epoch 7: Loss 5.6749\n",
      "  Skipgram Softmax (W=5) Epoch 8: Loss 5.6632\n",
      "  Skipgram Softmax (W=5) Epoch 9: Loss 5.6562\n",
      "  Skipgram Softmax (W=5) Epoch 10: Loss 5.6494\n",
      "  Skipgram Softmax (W=5) Epoch 11: Loss 5.6453\n",
      "  Skipgram Softmax (W=5) Epoch 12: Loss 5.6416\n",
      "  Skipgram Softmax (W=5) Epoch 13: Loss 5.6392\n",
      "  Skipgram Softmax (W=5) Epoch 14: Loss 5.6368\n",
      "  Skipgram Softmax (W=5) Epoch 15: Loss 5.6355\n",
      "  Skipgram Softmax (W=5) Epoch 16: Loss 5.6337\n",
      "  Skipgram Softmax (W=5) Epoch 17: Loss 5.6336\n",
      "  Skipgram Softmax (W=5) Epoch 18: Loss 5.6321\n",
      "  Skipgram Softmax (W=5) Epoch 19: Loss 5.6316\n",
      "  Skipgram Softmax (W=5) Epoch 20: Loss 5.6309\n",
      "  Skipgram NEG (W=5) Epoch 1: Loss 6.3494\n",
      "  Skipgram NEG (W=5) Epoch 2: Loss 2.1036\n",
      "  Skipgram NEG (W=5) Epoch 3: Loss 1.7847\n",
      "  Skipgram NEG (W=5) Epoch 4: Loss 1.6986\n",
      "  Skipgram NEG (W=5) Epoch 5: Loss 1.6585\n",
      "  Skipgram NEG (W=5) Epoch 6: Loss 1.6361\n",
      "  Skipgram NEG (W=5) Epoch 7: Loss 1.6171\n",
      "  Skipgram NEG (W=5) Epoch 8: Loss 1.6049\n",
      "  Skipgram NEG (W=5) Epoch 9: Loss 1.5929\n",
      "  Skipgram NEG (W=5) Epoch 10: Loss 1.5875\n",
      "  Skipgram NEG (W=5) Epoch 11: Loss 1.5776\n",
      "  Skipgram NEG (W=5) Epoch 12: Loss 1.5725\n",
      "  Skipgram NEG (W=5) Epoch 13: Loss 1.5677\n",
      "  Skipgram NEG (W=5) Epoch 14: Loss 1.5653\n",
      "  Skipgram NEG (W=5) Epoch 15: Loss 1.5586\n",
      "  Skipgram NEG (W=5) Epoch 16: Loss 1.5560\n",
      "  Skipgram NEG (W=5) Epoch 17: Loss 1.5530\n",
      "  Skipgram NEG (W=5) Epoch 18: Loss 1.5537\n",
      "  Skipgram NEG (W=5) Epoch 19: Loss 1.5508\n",
      "  Skipgram NEG (W=5) Epoch 20: Loss 1.5453\n",
      "  GloVe Scratch (W=5) Epoch 1: Loss 3.0681\n",
      "  GloVe Scratch (W=5) Epoch 2: Loss 1.1862\n",
      "  GloVe Scratch (W=5) Epoch 3: Loss 0.4112\n",
      "  GloVe Scratch (W=5) Epoch 4: Loss 0.1771\n",
      "  GloVe Scratch (W=5) Epoch 5: Loss 0.1051\n",
      "  GloVe Scratch (W=5) Epoch 6: Loss 0.0914\n",
      "  GloVe Scratch (W=5) Epoch 7: Loss 0.0923\n",
      "  GloVe Scratch (W=5) Epoch 8: Loss 0.0858\n",
      "  GloVe Scratch (W=5) Epoch 9: Loss 0.0745\n",
      "  GloVe Scratch (W=5) Epoch 10: Loss 0.0705\n",
      "  GloVe Scratch (W=5) Epoch 11: Loss 0.0669\n",
      "  GloVe Scratch (W=5) Epoch 12: Loss 0.0630\n",
      "  GloVe Scratch (W=5) Epoch 13: Loss 0.0588\n",
      "  GloVe Scratch (W=5) Epoch 14: Loss 0.0560\n",
      "  GloVe Scratch (W=5) Epoch 15: Loss 0.0527\n",
      "  GloVe Scratch (W=5) Epoch 16: Loss 0.0500\n",
      "  GloVe Scratch (W=5) Epoch 17: Loss 0.0473\n",
      "  GloVe Scratch (W=5) Epoch 18: Loss 0.0449\n",
      "  GloVe Scratch (W=5) Epoch 19: Loss 0.0425\n",
      "  GloVe Scratch (W=5) Epoch 20: Loss 0.0403\n",
      "\n",
      " TESTING WINDOW SIZE: 10 \n",
      "  Skipgram Softmax (W=10) Epoch 1: Loss 6.1899\n",
      "  Skipgram Softmax (W=10) Epoch 2: Loss 5.9598\n",
      "  Skipgram Softmax (W=10) Epoch 3: Loss 5.9032\n",
      "  Skipgram Softmax (W=10) Epoch 4: Loss 5.8732\n",
      "  Skipgram Softmax (W=10) Epoch 5: Loss 5.8553\n",
      "  Skipgram Softmax (W=10) Epoch 6: Loss 5.8434\n",
      "  Skipgram Softmax (W=10) Epoch 7: Loss 5.8356\n",
      "  Skipgram Softmax (W=10) Epoch 8: Loss 5.8301\n",
      "  Skipgram Softmax (W=10) Epoch 9: Loss 5.8257\n",
      "  Skipgram Softmax (W=10) Epoch 10: Loss 5.8227\n",
      "  Skipgram Softmax (W=10) Epoch 11: Loss 5.8207\n",
      "  Skipgram Softmax (W=10) Epoch 12: Loss 5.8188\n",
      "  Skipgram Softmax (W=10) Epoch 13: Loss 5.8174\n",
      "  Skipgram Softmax (W=10) Epoch 14: Loss 5.8164\n",
      "  Skipgram Softmax (W=10) Epoch 15: Loss 5.8154\n",
      "  Skipgram Softmax (W=10) Epoch 16: Loss 5.8151\n",
      "  Skipgram Softmax (W=10) Epoch 17: Loss 5.8145\n",
      "  Skipgram Softmax (W=10) Epoch 18: Loss 5.8145\n",
      "  Skipgram Softmax (W=10) Epoch 19: Loss 5.8138\n",
      "  Skipgram Softmax (W=10) Epoch 20: Loss 5.8141\n",
      "  Skipgram NEG (W=10) Epoch 1: Loss 4.8260\n",
      "  Skipgram NEG (W=10) Epoch 2: Loss 1.9826\n",
      "  Skipgram NEG (W=10) Epoch 3: Loss 1.8665\n",
      "  Skipgram NEG (W=10) Epoch 4: Loss 1.8226\n",
      "  Skipgram NEG (W=10) Epoch 5: Loss 1.7935\n",
      "  Skipgram NEG (W=10) Epoch 6: Loss 1.7665\n",
      "  Skipgram NEG (W=10) Epoch 7: Loss 1.7504\n",
      "  Skipgram NEG (W=10) Epoch 8: Loss 1.7394\n",
      "  Skipgram NEG (W=10) Epoch 9: Loss 1.7306\n",
      "  Skipgram NEG (W=10) Epoch 10: Loss 1.7269\n",
      "  Skipgram NEG (W=10) Epoch 11: Loss 1.7260\n",
      "  Skipgram NEG (W=10) Epoch 12: Loss 1.7272\n",
      "  Skipgram NEG (W=10) Epoch 13: Loss 1.7300\n",
      "  Skipgram NEG (W=10) Epoch 14: Loss 1.7328\n",
      "  Skipgram NEG (W=10) Epoch 15: Loss 1.7350\n",
      "  Skipgram NEG (W=10) Epoch 16: Loss 1.7369\n",
      "  Skipgram NEG (W=10) Epoch 17: Loss 1.7449\n",
      "  Skipgram NEG (W=10) Epoch 18: Loss 1.7454\n",
      "  Skipgram NEG (W=10) Epoch 19: Loss 1.7481\n",
      "  Skipgram NEG (W=10) Epoch 20: Loss 1.7537\n",
      "  GloVe Scratch (W=10) Epoch 1: Loss 2.4548\n",
      "  GloVe Scratch (W=10) Epoch 2: Loss 0.8163\n",
      "  GloVe Scratch (W=10) Epoch 3: Loss 0.2307\n",
      "  GloVe Scratch (W=10) Epoch 4: Loss 0.0952\n",
      "  GloVe Scratch (W=10) Epoch 5: Loss 0.0665\n",
      "  GloVe Scratch (W=10) Epoch 6: Loss 0.0646\n",
      "  GloVe Scratch (W=10) Epoch 7: Loss 0.0569\n",
      "  GloVe Scratch (W=10) Epoch 8: Loss 0.0522\n",
      "  GloVe Scratch (W=10) Epoch 9: Loss 0.0474\n",
      "  GloVe Scratch (W=10) Epoch 10: Loss 0.0451\n",
      "  GloVe Scratch (W=10) Epoch 11: Loss 0.0410\n",
      "  GloVe Scratch (W=10) Epoch 12: Loss 0.0392\n",
      "  GloVe Scratch (W=10) Epoch 13: Loss 0.0359\n",
      "  GloVe Scratch (W=10) Epoch 14: Loss 0.0346\n",
      "  GloVe Scratch (W=10) Epoch 15: Loss 0.0320\n",
      "  GloVe Scratch (W=10) Epoch 16: Loss 0.0307\n",
      "  GloVe Scratch (W=10) Epoch 17: Loss 0.0286\n",
      "  GloVe Scratch (W=10) Epoch 18: Loss 0.0276\n",
      "  GloVe Scratch (W=10) Epoch 19: Loss 0.0259\n",
      "  GloVe Scratch (W=10) Epoch 20: Loss 0.0250\n"
     ]
    }
   ],
   "source": [
    "# EXECUTION LOOP FOR MULTIPLE WINDOW SIZES \n",
    "\n",
    "WINDOW_SIZES = [2, 5, 10]\n",
    "corpus, vocab, word2idx = load_news_dataset()\n",
    "i2word = {i: w for w, i in word2idx.items()}\n",
    "analogy_path = download_analogy_data()\n",
    "performance_results = []\n",
    "judgment_results = {}\n",
    "\n",
    "for window in WINDOW_SIZES:\n",
    "    print(f\"\\n TESTING WINDOW SIZE: {window} \")\n",
    "    \n",
    "    models = [\n",
    "        (\"Skipgram Softmax\", SkipgramSoftmax(len(vocab), 100)),\n",
    "        (\"Skipgram NEG\", Word2VecNeg(len(vocab), 100)),\n",
    "        (\"GloVe Scratch\", GloVeModel(len(vocab), 100))\n",
    "    ]\n",
    "\n",
    "    for name, model in models:\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Dynamic Data Generation and Training with Epoch Logging\n",
    "        if \"Skipgram\" in name:\n",
    "            pairs = get_skipgrams(corpus, word2idx, window)\n",
    "            criterion = nn.CrossEntropyLoss() if \"Softmax\" in name else None\n",
    "            for epoch in range(20): \n",
    "                total_loss = 0\n",
    "                for i in range(0, len(pairs), 128):\n",
    "                    batch = pairs[i:i+128]\n",
    "                    if len(batch) < 128: continue\n",
    "                    c, t = torch.LongTensor([p[0] for p in batch]), torch.LongTensor([p[1] for p in batch])\n",
    "                    optimizer.zero_grad()\n",
    "                    loss = model(c, t, torch.LongTensor(np.random.randint(0, len(vocab), (128, 5)))) if \"NEG\" in name else criterion(model(c), t)\n",
    "                    loss.backward(); optimizer.step(); total_loss += loss.item()\n",
    "                avg_loss = total_loss / (len(pairs)//128)\n",
    "                print(f\"  {name} (W={window}) Epoch {epoch+1}: Loss {avg_loss:.4f}\")\n",
    "        else: # GloVe\n",
    "            cooc = get_cooc_matrix(corpus, word2idx, window)\n",
    "            for epoch in range(20):\n",
    "                total_loss = 0\n",
    "                for i in range(0, len(cooc), 128):\n",
    "                    batch = cooc[i:i+128]\n",
    "                    if len(batch) < 128: continue\n",
    "                    idx_i, idx_j = torch.LongTensor([p[0] for p in batch]), torch.LongTensor([p[1] for p in batch])\n",
    "                    cnt, wgt = torch.FloatTensor([p[2] for p in batch]), torch.FloatTensor([p[3] for p in batch])\n",
    "                    optimizer.zero_grad(); loss = model(idx_i, idx_j, cnt, wgt); loss.backward(); optimizer.step(); total_loss += loss.item()\n",
    "                avg_loss = total_loss / (len(cooc)//128)\n",
    "                print(f\"  {name} (W={window}) Epoch {epoch+1}: Loss {avg_loss:.4f}\")\n",
    "\n",
    "        duration = time.time() - start_time\n",
    "        corr = evaluate_spearman(model, word2idx, 'combined.csv')\n",
    "        \n",
    "        # Extract Vectors for Analogy Task\n",
    "        if \"NEG\" in name: vecs = model.v_embeddings.weight.detach().numpy()\n",
    "        elif \"GloVe\" in name: vecs = (model.v_emb.weight + model.u_emb.weight).detach().numpy()\n",
    "        else: vecs = model.embeddings.weight.detach().numpy()\n",
    "\n",
    "        syntactic = evaluate_analogy(vecs, word2idx, i2word, analogy_path, \"past-tense\")\n",
    "        semantic = evaluate_analogy(vecs, word2idx, i2word, analogy_path, \"capital-common-countries\")\n",
    "        \n",
    "        performance_results.append({\n",
    "            \"Model\": name, \"Window\": window, \n",
    "            \"Training Time\": f\"{duration:.2f}s\", \"Spearman Rho\": f\"{corr:.4f}\",\n",
    "            \"Syntactic Accuracy\": f\"{syntactic:.2f}%\", \"Semantic Accuracy\": f\"{semantic:.2f}%\"\n",
    "        })\n",
    "        \n",
    "        if name not in judgment_results or corr > judgment_results[name]:\n",
    "            judgment_results[name] = corr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afc71746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating GloVe (Gensim)\n"
     ]
    }
   ],
   "source": [
    "# Pre-trained Gensim Benchmark\n",
    "print(\"\\nEvaluating GloVe (Gensim)\")\n",
    "g_g = api.load(\"glove-wiki-gigaword-100\")\n",
    "g_corr = evaluate_spearman(g_g, None, 'combined.csv', is_gensim=True)\n",
    "g_syn = evaluate_analogy(g_g.vectors, g_g.key_to_index, {v:k for k,v in g_g.key_to_index.items()}, analogy_path, \"past-tense\")\n",
    "g_sem = evaluate_analogy(g_g.vectors, g_g.key_to_index, {v:k for k,v in g_g.key_to_index.items()}, analogy_path, \"capital-common-countries\")\n",
    "\n",
    "performance_results.append({\n",
    "    \"Model\": \"GloVe (gensim)\", \"Window\": \"N/A\", \"Training Time\": \"N/A\", \"Spearman Rho\": f\"{g_corr:.4f}\",\n",
    "    \"Syntactic Accuracy\": f\"{g_syn:.2f}%\", \"Semantic Accuracy\": f\"{g_sem:.2f}%\"\n",
    "})\n",
    "judgment_results[\"GloVe (gensim)\"] = g_corr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2e45553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " PERFORMANCE SUMMARY TABLE \n",
      "           Model Window Training Time Spearman Rho Syntactic Accuracy Semantic Accuracy\n",
      "Skipgram Softmax      2       182.89s      -0.1652              2.38%             0.00%\n",
      "    Skipgram NEG      2       152.94s      -0.0452              2.38%             0.00%\n",
      "   GloVe Scratch      2        35.95s      -0.1397              0.00%             0.00%\n",
      "Skipgram Softmax      5       428.92s       0.1843              0.00%             0.00%\n",
      "    Skipgram NEG      5       351.88s       0.0136              0.00%             0.00%\n",
      "   GloVe Scratch      5        78.62s      -0.1365              0.00%             0.00%\n",
      "Skipgram Softmax     10       732.30s       0.3189              0.00%             0.00%\n",
      "    Skipgram NEG     10       637.84s      -0.2813              0.00%             0.00%\n",
      "   GloVe Scratch     10       128.10s       0.0319              0.00%             0.00%\n",
      "  GloVe (gensim)    N/A           N/A       0.4924             53.40%            94.07%\n",
      "\n",
      " TABLE 1: ASSESSMENT OF HUMAN JUDGMENT CORRELATION\n",
      "              Skipgram       NEG     GloVe  GloVe (gensim)  Y_true\n",
      "Spearman Rho  0.318872  0.013646  0.031926        0.492414     1.0\n"
     ]
    }
   ],
   "source": [
    "# DISPLAY RESULTS\n",
    "print(\"\\n PERFORMANCE SUMMARY TABLE \")\n",
    "print(pd.DataFrame(performance_results).to_string(index=False))\n",
    "\n",
    "swapped_data = [[\n",
    "    judgment_results.get(\"Skipgram Softmax\", 0),\n",
    "    judgment_results.get(\"Skipgram NEG\", 0),\n",
    "    judgment_results.get(\"GloVe Scratch\", 0),\n",
    "    judgment_results.get(\"GloVe (gensim)\", 0),\n",
    "    1.0 # Y_true benchmark\n",
    "]]\n",
    "print(\"\\n TABLE 1: ASSESSMENT OF HUMAN JUDGMENT CORRELATION\")\n",
    "print(pd.DataFrame(swapped_data, columns=[\"Skipgram\", \"NEG\", \"GloVe\", \"GloVe (gensim)\", \"Y_true\"], index=[\"Spearman Rho\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
