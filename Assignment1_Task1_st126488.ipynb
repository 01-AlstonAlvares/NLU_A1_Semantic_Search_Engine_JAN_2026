{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e5d5735",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     C:\\Users\\alsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\alsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "\n",
    "# 0. SETUP & DATA SOURCING\n",
    "# Proper credit: Reuters-21578 Text Categorization Test Collection\n",
    "try:\n",
    "    nltk.data.find('corpora/reuters')\n",
    "except LookupError:\n",
    "    nltk.download('reuters')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "def load_news_dataset(category='grain', max_vocab=5000):\n",
    "    \"\"\"Loads and cleans real-world news data from NLTK.\"\"\"\n",
    "    raw_sents = reuters.sents(categories=category)\n",
    "    # Clean: lowercasing and removing punctuation/numbers\n",
    "    clean_corpus = [[w.lower() for w in s if w.isalpha()] for s in raw_sents]\n",
    "    \n",
    "    # Vocabulary building\n",
    "    all_words = [w for s in clean_corpus for w in s]\n",
    "    counts = Counter(all_words)\n",
    "    vocab = sorted(counts, key=counts.get, reverse=True)[:max_vocab]\n",
    "    vocab.append('<UNK>')\n",
    "    \n",
    "    word2idx = {w: i for i, w in enumerate(vocab)}\n",
    "    idx2word = {i: w for i, w in enumerate(vocab)}\n",
    "    \n",
    "    return clean_corpus, vocab, word2idx, idx2word\n",
    "\n",
    "# Initialize Data\n",
    "corpus, vocab, word2index, index2word = load_news_dataset()\n",
    "voc_size = len(vocab)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28758fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. DYNAMIC UTILITIES\n",
    "def get_skipgrams(corpus, w2i, window_size=2):\n",
    "    \"\"\"Generates training pairs with a dynamic window size.\"\"\"\n",
    "    data = []\n",
    "    for sentence in corpus:\n",
    "        for i, word in enumerate(sentence):\n",
    "            if word not in w2i: continue\n",
    "            target = w2i[word]\n",
    "            \n",
    "            # Dynamic window bounds\n",
    "            start = max(0, i - window_size)\n",
    "            end = min(len(sentence), i + window_size + 1)\n",
    "            \n",
    "            for j in range(start, end):\n",
    "                if i == j or sentence[j] not in w2i: continue\n",
    "                data.append((target, w2i[sentence[j]]))\n",
    "    return data\n",
    "\n",
    "def get_unigram_table(vocab, corpus, w2i):\n",
    "    \"\"\"Creates a noise distribution for negative sampling (P(w)^3/4).\"\"\"\n",
    "    counts = Counter([w for s in corpus for w in s if w in w2i])\n",
    "    total = sum(counts.values())\n",
    "    table = []\n",
    "    for word in vocab:\n",
    "        if word == '<UNK>': continue\n",
    "        # Mikolov's heuristic for negative sampling\n",
    "        freq = int(((counts[word]/total)**0.75) / 0.001)\n",
    "        table.extend([word] * max(freq, 1))\n",
    "    return table\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87f2e74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. WORD2VEC (NEGATIVE SAMPLING)\n",
    "class Word2VecNeg(nn.Module):\n",
    "    def __init__(self, v_size, emb_dim):\n",
    "        super(Word2VecNeg, self).__init__()\n",
    "        self.v_embeddings = nn.Embedding(v_size, emb_dim) # Center\n",
    "        self.u_embeddings = nn.Embedding(v_size, emb_dim) # Outside\n",
    "        self.log_sigmoid = nn.LogSigmoid()\n",
    "        \n",
    "    def forward(self, center, target, negative):\n",
    "        # Reshaping to ensure [Batch, 1, Emb] to avoid IndexErrors\n",
    "        v_vecs = self.v_embeddings(center).view(center.size(0), 1, -1)\n",
    "        u_vecs = self.u_embeddings(target).view(target.size(0), 1, -1)\n",
    "        n_vecs = -self.u_embeddings(negative) # [Batch, K, Emb]\n",
    "        \n",
    "        # Positive score: dot product of center and target\n",
    "        pos_score = torch.bmm(u_vecs, v_vecs.transpose(1, 2)).view(center.size(0), -1)\n",
    "        # Negative score: dot product of center and K noise samples\n",
    "        neg_score = torch.bmm(n_vecs, v_vecs.transpose(1, 2)).squeeze(2)\n",
    "        \n",
    "        loss = self.log_sigmoid(pos_score) + torch.sum(self.log_sigmoid(neg_score), 1)\n",
    "        return -torch.mean(loss)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c2a8e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. GLOVE IMPLEMENTATION\n",
    "class GloVeModel(nn.Module):\n",
    "    def __init__(self, v_size, emb_dim):\n",
    "        super(GloVeModel, self).__init__()\n",
    "        self.v_emb = nn.Embedding(v_size, emb_dim)\n",
    "        self.u_emb = nn.Embedding(v_size, emb_dim)\n",
    "        self.v_bias = nn.Embedding(v_size, 1)\n",
    "        self.u_bias = nn.Embedding(v_size, 1)\n",
    "        \n",
    "    def forward(self, i_indices, j_indices, cooc_counts, weights):\n",
    "        v_vec = self.v_emb(i_indices)\n",
    "        u_vec = self.u_emb(j_indices)\n",
    "        b_i = self.v_bias(i_indices).squeeze(1)\n",
    "        b_j = self.u_bias(j_indices).squeeze(1)\n",
    "        \n",
    "        # Dot product\n",
    "        dot = (v_vec * u_vec).sum(1)\n",
    "        # Main GloVe Objective\n",
    "        loss = weights * torch.pow(dot + b_i + b_j - torch.log(cooc_counts), 2)\n",
    "        return torch.mean(loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efeef2b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec (NEG) on Reuters Category: 'grain'\n",
      "Total pairs: 352144 | Window Size: 2\n"
     ]
    }
   ],
   "source": [
    "# 4. FULL TRAINING EXECUTION\n",
    "\n",
    "# Hyperparameters\n",
    "WINDOW_SIZE = 2 \n",
    "EMB_DIM = 100\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS = 15 \n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "# Prepare Data\n",
    "pairs = get_skipgrams(corpus, word2index, window_size=WINDOW_SIZE)\n",
    "noise_table = get_unigram_table(vocab, corpus, word2index)\n",
    "\n",
    "# Initialize Model & Optimizer\n",
    "model = Word2VecNeg(voc_size, EMB_DIM)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "def sample_negative(targets, table, k, w2i):\n",
    "    \"\"\"Helper to pick negative indices.\"\"\"\n",
    "    batch_size = targets.size(0)\n",
    "    negs = []\n",
    "    for i in range(batch_size):\n",
    "        s = []\n",
    "        while len(s) < k:\n",
    "            pick = random.choice(table)\n",
    "            if w2i[pick] == targets[i].item(): continue\n",
    "            s.append(w2i[pick])\n",
    "        negs.append(torch.LongTensor(s).view(1, -1))\n",
    "    return torch.cat(negs)\n",
    "\n",
    "print(f\"Training Word2Vec (NEG) on Reuters Category: 'grain'\")\n",
    "print(f\"Total pairs: {len(pairs)} | Window Size: {WINDOW_SIZE}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66617b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15] - Average Loss: 20.2847\n",
      "Epoch [2/15] - Average Loss: 14.8388\n",
      "Epoch [3/15] - Average Loss: 11.1705\n",
      "Epoch [4/15] - Average Loss: 8.4469\n",
      "Epoch [5/15] - Average Loss: 6.5545\n",
      "Epoch [6/15] - Average Loss: 5.2548\n",
      "Epoch [7/15] - Average Loss: 4.3565\n",
      "Epoch [8/15] - Average Loss: 3.7561\n",
      "Epoch [9/15] - Average Loss: 3.3312\n",
      "Epoch [10/15] - Average Loss: 3.0127\n",
      "Epoch [11/15] - Average Loss: 2.7689\n",
      "Epoch [12/15] - Average Loss: 2.5746\n",
      "Epoch [13/15] - Average Loss: 2.4191\n",
      "Epoch [14/15] - Average Loss: 2.2873\n",
      "Epoch [15/15] - Average Loss: 2.1816\n",
      "\n",
      "Training Complete!\n",
      "Total time taken: 261.11 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_loss = 0\n",
    "    random.shuffle(pairs) # Shuffle for each epoch\n",
    "    \n",
    "    for i in range(0, len(pairs), BATCH_SIZE):\n",
    "        batch = pairs[i : i + BATCH_SIZE]\n",
    "        if len(batch) < BATCH_SIZE: continue\n",
    "            \n",
    "        centers = torch.LongTensor([p[0] for p in batch]) # Center words\n",
    "        targets = torch.LongTensor([p[1] for p in batch]) # Outside words\n",
    "        negatives = sample_negative(targets, noise_table, 5, word2index)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = model(centers, targets, negatives) # Compute loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    avg_loss = epoch_loss / (len(pairs) // BATCH_SIZE)\n",
    "    print(f\"Epoch [{epoch+1}/{EPOCHS}] - Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"\\nTraining Complete!\")\n",
    "print(f\"Total time taken: {end_time - start_time:.2f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
